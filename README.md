# Bank-Churning-ML-Analysis
This project focuses on predicting customer churn using the Bank Customer Churn dataset from Kaggle. Various machine learning models, including Logistic Regression, Decision Tree Classifier, and Random Forest Classifier, were applied to understand the factors influencing churn and build effective predictive models.

The dataset features include customer credit score, age, tenure, balance, and product number. Notable patterns emerged from the analysis: the distribution of credit scores was slightly skewed left, with a majority of customers falling into the subprime, near prime, and prime categories (mean of 650-750). Age was skewed to the right, indicating that most customers were between 30 and 50 years old. Most customers had been with the bank for 1 to 9 years, with a sharp drop-off in tenure beyond 9 years. A significant proportion of customers had zero balance in their accounts, while the remaining balances followed a normal distribution. Most customers held 1-2 products, and estimated salaries were evenly distributed across the 0 to 200,000 range.

Exploratory data analysis (EDA) revealed that churn was more common among customers aged 40-50, with German customers having the highest churn rate compared to other countries. No significant correlations were found between most variables, although age and churn showed some moderate correlation.

The project applied three machine learning models. Initially, the Logistic Regression model achieved an accuracy of 80%, though it struggled with predicting customer churn due to data imbalance. After applying the SMOTE (Synthetic Minority Over-sampling Technique) algorithm to balance the data, the model's recall for churned customers improved to 86%. Next, the Decision Tree Classifier proved to be effective for this binary dataset, with an accuracy of 86%. Finally, the Random Forest Classifier, which aggregates multiple decision trees, outperformed the other models with an accuracy of 90%. This model demonstrated the benefits of ensemble methods, reducing overfitting and better handling outliers and missing data.

In conclusion, the Random Forest Classifier was the most effective model, achieving a 90% accuracy after oversampling and data preprocessing. This project highlights the importance of handling data imbalance and the advantages of ensemble methods in improving model performance. Detailed analysis and code implementation can be found in the project repository.
